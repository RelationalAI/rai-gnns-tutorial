{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6fb832-e2d3-48c9-b669-b7a1a248b0bb",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# HM Churn Prediction: Node Classification example using the GNN learning engine under the RelationalAI Snowflake Native App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7332859-144c-43c5-b0ce-eadc96928183",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "### üîê Connecting to the RelationalAI Native App via Snowflake\n",
    "\n",
    "To connect to the RelationalAI native app, we'll first set up a dictionary that defines the necessary environment variables for establishing a Snowflake connection. Here we will use\n",
    "the `active_session` authentication method which gets all the credentials needed from the\n",
    "current snowflake session. We only need to provide the application name (`RELATIONALAI`).\n",
    "\n",
    "#### üîê About Authentication\n",
    "\n",
    "In this tutorial, we‚Äôll use **active_session**. However, other authentication methods‚Äîsuch as **Key Pair Authentication**, **Password** or **OAuth Token Authentication**‚Äîare also supported. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb4c47-3238-4a33-b28e-b16aa3cb73a4",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# load the snowflake configuration to a python dict\n",
    "snowflake_config = {\n",
    "    \"app_name\": \"RELATIONALAI\",\n",
    "    \"auth_method\": \"active_session\",\n",
    "    \"role\": \"SYSADMIN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9502d1-6af8-48ac-8b71-b4878bee3e5a",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## ‚öôÔ∏è Managing Your GNN Engines\n",
    "\n",
    "The `Provider` class in the RelationalAI GNN SDK allows you to manage your GNN engines seamlessly. Below, we walk through common operations you can perform with the `Provider`:\n",
    "\n",
    "* ‚úÖ Create a new GNN engine\n",
    "* üìã List all available engines\n",
    "* üîç Check the status of an engine\n",
    "* üîÑ Resume a paused engine\n",
    "* ‚ùå Delete an engine\n",
    "\n",
    "Each of these operations can be done with simple method calls, as shown in the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "# SDK imports\n",
    "from rai_gnns_experimental import ColumnDType\n",
    "from rai_gnns_experimental import EvaluationMetric\n",
    "from rai_gnns_experimental import GNNTable, ForeignKey\n",
    "from rai_gnns_experimental import NodeTask, TaskType\n",
    "from rai_gnns_experimental import Trainer\n",
    "from rai_gnns_experimental import TrainerConfig\n",
    "from rai_gnns_experimental import Dataset\n",
    "from rai_gnns_experimental import JobManager\n",
    "from rai_gnns_experimental import OutputConfig, SnowflakeConnector, Provider\n",
    "from rai_gnns_experimental import ExperimentConfig, ModelManager\n",
    "\n",
    "\n",
    "from graphviz import Source\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7860564-0bb2-4fc9-aec8-70edcf2a4813",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "# engine setup\n",
    "engine_name = \"my_engine\"\n",
    "engine_size = \"GPU_NV_S\" # Available sizes: \"GPU_NV_S\" or \"HIGHMEM_X64_S\"\n",
    "\n",
    "# database /s chema\n",
    "DB_NAME = \"HM_DB\"\n",
    "DB_SCHEMA_NAME = \"HM_SCHEMA\"\n",
    "TASK_SCHEMA_NAME = \"HM_CHURN\"\n",
    "DB_SCHEMA = f\"{DB_NAME}.{DB_SCHEMA_NAME}\"\n",
    "TASK_SCHEMA = f\"{DB_NAME}.{TASK_SCHEMA_NAME}\"\n",
    "MODEL_REGISTRY_SCHEMA = \"model_registry\"\n",
    "\n",
    "# dataset name\n",
    "DATASET_NAME = \"HM_Dataset\"\n",
    "\n",
    "# customer data\n",
    "CUSTOMER_SOURCE_TABLE = f\"{DB_SCHEMA}.CUSTOMERS\"\n",
    "CUSTOMER_NAME = \"customers\"\n",
    "CUSTOMER_PRIMARY_KEY = \"customer_id\"\n",
    "\n",
    "# article data\n",
    "ARTICLE_SOURCE_TABLE = f\"{DB_SCHEMA}.ARTICLES\"\n",
    "ARTICLE_NAME = \"articles\"\n",
    "ARTICLE_PRIMARY_KEY = \"article_id\"\n",
    "\n",
    "# transaction data\n",
    "TRANSACTION_SOURCE_TABLE = f\"{DB_SCHEMA}.TRANSACTIONS\"\n",
    "TRANSACTION_NAME = \"transactions\"\n",
    "TIME_COLUMN = \"t_dat\"\n",
    "\n",
    "# node task\n",
    "NODE_TASK_NAME = \"churn_task\"\n",
    "TASK_TARGET_COLUMN_NAME = \"churn\"\n",
    "TASK_TIME_COLUMN_NAME = \"timestamp\"\n",
    "TASK_TRAIN_TABLE = f\"{TASK_SCHEMA}.TRAIN\"\n",
    "TASK_TEST_TABLE  = f\"{TASK_SCHEMA}.TEST\"\n",
    "TASK_VALIDATION_TABLE = f\"{TASK_SCHEMA}.VALIDATION\"\n",
    "\n",
    "# model params\n",
    "MODEL_DEVICE = \"cuda\" # either 'cuda' or 'cpu'\n",
    "MODEL_N_EPOCHS = 3\n",
    "MODEL_MAX_ITERS = 200\n",
    "MODEL_TEXT_EMBEDDER = \"model2vec-potion-base-4M\"\n",
    "\n",
    "\n",
    "OUTPUT_ALIAS = \"TEST_PREDS_1\"\n",
    "OUTPUT_TABLE = f\"PREDICTIONS_{OUTPUT_ALIAS}\"\n",
    "OUTPUT_TABLE_EMB = f\"PREDICTIONS_EMBEDDINGS_CUSTOMER_ID_{OUTPUT_ALIAS}\"\n",
    "OUTPUT_ALIAS_BEST = \"TEST_PREDS_BEST\"\n",
    "REGISTERED_MODEL_NAME = \"test_model_mlflow\"\n",
    "OUTPUT_ALIAS_REGISTERED = \"TEST_PREDS_REGISTERED\"\n",
    "TEST_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc64f3d-7243-413b-a0bc-bd8b54f0899f",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# initialize the provider using the snowflake configuration\n",
    "# (note: you might be prompted from your MFA app at this point)\n",
    "provider = Provider(**snowflake_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cfc37b-382f-47e4-bcd5-a7b29bd891f1",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# Create a new GNN engine.\n",
    "# Currently supported engine types:\n",
    "# - For Snowflake accounts hosted on AWS: \"GPU_NV_S\" and \"HIGHMEM_X64_S\"\n",
    "# - For Snowflake accounts hosted on Azure: \"HIGHMEM_X64_S\" only\n",
    "#\n",
    "# You must provide:\n",
    "# - A custom engine name (via the `name` parameter)\n",
    "# - The engine type (via the `size` parameter)\n",
    "#\n",
    "# Available sizes:\n",
    "# - AWS: \"GPU_NV_S\", \"HIGHMEM_X64_S\"\n",
    "# - Azure: \"HIGHMEM_X64_S\"\n",
    "#\n",
    "# provider.create_gnn(\n",
    "#    name=engine_name,\n",
    "#    size=\"GPU_NV_S\"  # or \"HIGHMEM_X64_S\"\n",
    "#)\n",
    "\n",
    "# The creation of a new engine may take \n",
    "# several minutes to complete (e.g., ~4 minutes).\n",
    "# Be patient and do not interrupt the process.\n",
    "\n",
    "# check if engine exists, if yes, resume if not create it\n",
    "if not provider.get_gnn(engine_name): \n",
    "    print(f\"Creating Engine {engine_name}\")\n",
    "    provider.create_gnn(name=engine_name, size=engine_size)\n",
    "else:\n",
    "    print(f\"Resuming Engine {engine_name}\")\n",
    "    provider.resume_gnn(name=engine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff3fac-d126-4aa2-b047-609071683c43",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# HINT: We can always resume a GNN engine that has been suspended:\n",
    "# Note: Engine provisioning can take some minutes. Please\n",
    "# check the engine status using provider.get_gnn(name=\"my_engine\")\n",
    "\n",
    "# provider.resume_gnn(name=engine_name)\n",
    "\n",
    "# And if we need we can also delete a GNN engine\n",
    "\n",
    "# provider.delete_gnn(name=engine_name)\n",
    "\n",
    "# we can also check the existing engines. If there is not engines listed here you would need to create one using provider.create_gnn(name=\"my_engine\", size=\"GPU_NV_S\")\n",
    "# provider.list_gnns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb00f98-465f-4db1-92ae-1b93a3f95779",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# If we resume an engine, we can directly see the status of the engine\n",
    "# the status of the engine 'READY' marks the fact that the\n",
    "# engine is ready to be used. A `PENDING` status marks the\n",
    "# fact that the engine has  been automaticaly suspended.\n",
    "# Notice also that under the settings \n",
    "# the provider exposes a URL for the MLFLOW endpoint\n",
    "# that we can use to track our experiments\n",
    "# NOTE: we should wait until the status is READY\n",
    "\n",
    "engine_data = provider.get_gnn(engine_name)\n",
    "if engine_data[\"state\"] == 'SUSPENDED':\n",
    "    print(f'ENGINE {engine_name} IS SUSPENDED, YOU HAVE TO RESUME IT FIRST')\n",
    "else:\n",
    "    while not engine_data or engine_data[\"state\"] != 'READY':\n",
    "        time.sleep(10)\n",
    "        engine_data = provider.get_gnn(engine_name) \n",
    "    \n",
    "\n",
    "    print(f'ENGINE {engine_name} READY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0709b-e05b-4c75-8d93-717f0fdba414",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "‚ö†Ô∏è **Warning:** To  make sure to check the engine status (e.g., via `provider.get_gnn(\"engine_name\")`) and make sure that the status is `READY` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623023d0-8df9-4dae-af5a-66c27acff735",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "## üîå Connector Setup\n",
    "\n",
    "The `Connector` class, like the `Provider` class, is used to communicate with Snowflake. However, while the `Provider` is responsible for managing GNN engines, the `Connector` is specifically used to interface with the **GNN learning engine** itself.\n",
    "\n",
    "You‚Äôll use the `Connector` instance as an input to all SDK components that need to send requests to the GNN engine‚Äîsuch as loading data, running training jobs, or performing inference.\n",
    "\n",
    "In short:\n",
    "\n",
    "* `Provider` ‚Üí Manages GNN engine instances (create, list, delete, etc.)\n",
    "* `Connector` ‚Üí Sends requests to a specific GNN engine for processing tasks\n",
    "\n",
    "Let‚Äôs now walk through how to create and use a `Connector`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afaceae-881c-4cf6-b2c0-de42ac1b9542",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# we initialize the connector and passing all our credentials.\n",
    "connector = SnowflakeConnector(\n",
    "    **snowflake_config,\n",
    "    engine_name=engine_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cb986-d914-42f6-af2d-d0d5c5ec4e1c",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "## üìä Preparing the data: Creating the GNN tables\n",
    "\n",
    "In this section, we will define the GNN tables and the associated learning task. These components will then be used to construct a GNN dataset suitable for training.\n",
    "\n",
    "For this tutorial, we‚Äôll use the H&M database as our working example. This database includes three tables: CUSTOMERS, ARTICLES, and TRANSACTIONS. The TRANSACTIONS table links CUSTOMERS to ARTICLES. Our objective is to predict if a given CUSTOMER is going to churn in the next week, meaning if they are going to stop making any TRANSACTIONS. We handle this problem as a binary node classification task (churn=1/no churn=0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98f931-2205-4d42-bfa8-7d8a1eb38038",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "# create a table for the customers and set the \n",
    "# customer_id as a primary key (primary and \n",
    "# foreign keys are used to construct the edges of the graph)\n",
    "customers_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=CUSTOMER_SOURCE_TABLE,\n",
    "    name=CUSTOMER_NAME,\n",
    "    primary_key=CUSTOMER_PRIMARY_KEY,\n",
    ")\n",
    "customers_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11a6080-78b8-4732-8089-86772ca5a718",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# in a similar manner we can create the ARTICLES table\n",
    "articles_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=ARTICLE_SOURCE_TABLE,\n",
    "    name=ARTICLE_NAME,\n",
    "    primary_key=ARTICLE_PRIMARY_KEY,\n",
    ")\n",
    "articles_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db9fa-2d55-40be-aae3-88fdcceabb4e",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# and finally we will link the two tables using the foreign\n",
    "# keys from the TRANSACTIONS table.\n",
    "# Note that the date column (`t_dat`) is important. \n",
    "# In a typical scenario where one would use shallow models (e.g,. XGBoost) \n",
    "# to construct a feature matrix, special care \n",
    "# needs to be taken to avoid time leakage \n",
    "# (for example, by masking future transactions). \n",
    "# By specifying the date column as a time column, \n",
    "# GNNs take care of the time leakage problems.\n",
    "\n",
    "transactions_table = GNNTable(\n",
    "    connector=connector,\n",
    "    source=TRANSACTION_SOURCE_TABLE,\n",
    "    name=TRANSACTION_NAME,\n",
    "    foreign_keys=[\n",
    "        ForeignKey(\n",
    "            column_name=CUSTOMER_PRIMARY_KEY, link_to=CUSTOMER_NAME+\".\"+CUSTOMER_PRIMARY_KEY),\n",
    "        ForeignKey(\n",
    "            column_name=ARTICLE_PRIMARY_KEY, link_to=ARTICLE_NAME+\".\"+ARTICLE_PRIMARY_KEY),\n",
    "    ],\n",
    "    time_column=TIME_COLUMN,\n",
    ")\n",
    "transactions_table.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca4b6fb-9336-4836-88d3-8c6914b071ab",
   "metadata": {
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "\n",
    "## üîß Preparing the Data: Creating the Task\n",
    "\n",
    "To define the task, we begin by specifying the locations of the training, validation, and test datasets. We also identify the source table, along with the corresponding columns that uniquely identify each entity.\n",
    "\n",
    "Since this is a **binary node classification** task, our objective is to predict the label of each source entity.\n",
    "\n",
    "Additionally, we define a **timestamp column** to avoid information leakage by ensuring that future data doesn't influence past predictions. Lastly, we specify the **evaluation metric**‚Äîin this case, **ROC AUC**‚Äîto assess the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6393dc7-0391-4db9-874f-36ef45295860",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "churn_task = NodeTask(\n",
    "    connector=connector,\n",
    "    name=NODE_TASK_NAME,\n",
    "    # define train, validation and test (optional) tables\n",
    "    task_data_source={\n",
    "        \"train\": TASK_TRAIN_TABLE,\n",
    "        \"validation\": TASK_VALIDATION_TABLE,\n",
    "        \"test\": TASK_TEST_TABLE,\n",
    "    },\n",
    "    # name of source entity column that we want to do predictions for\n",
    "    source_entity_column=CUSTOMER_PRIMARY_KEY,\n",
    "    # name of GNN table that column is at\n",
    "    source_entity_table=CUSTOMER_NAME,\n",
    "    # name of the column that holds the info that we want to predict from our train table\n",
    "    target_column=TASK_TARGET_COLUMN_NAME,\n",
    "    time_column=TASK_TIME_COLUMN_NAME,\n",
    "    task_type=TaskType.BINARY_CLASSIFICATION,\n",
    "    evaluation_metric=EvaluationMetric(name=\"roc_auc\"),\n",
    ")\n",
    "\n",
    "churn_task.show_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d398c-72d4-44eb-8aea-b0cde2d17d7a",
   "metadata": {
    "collapsed": false,
    "name": "cell20"
   },
   "source": [
    "## üß© Preparing the Data: Creating the Dataset\n",
    "\n",
    "Finally, we combine all the components by constructing a dataset object that encapsulates both the GNN tables and the task definition. This dataset will serve as the input to the model training pipeline, ensuring that the task and its associated data are tightly integrated and ready for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce110000-1111-2222-3333-ffffff000009",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    connector=connector,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    tables=[articles_table, customers_table, transactions_table],\n",
    "    task_description=churn_task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868eb50-20f1-490b-970e-20a01454fa3d",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# we can also visualize the dataset \n",
    "graph = dataset.visualize_dataset(show_dtypes=True)\n",
    "# play with font size and plot size to get a good visualization\n",
    "for node in graph.get_nodes():    \n",
    "    font_size = node.get_attributes()['fontsize']\n",
    "    font_size = \"16\"\n",
    "    node.set('fontsize', font_size)\n",
    "\n",
    "graph.set_graph_defaults(size=\"10,10!\")  # Increase graph size\n",
    "\n",
    "src = Source(graph.to_string())\n",
    "src  # Display in notebo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcfc33d-ed25-4c37-a6c5-35feb0066b70",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "## üöÄ GNN Model Training\n",
    "\n",
    "Now that our dataset is ready, we can train our first GNN model. We‚Äôll begin by defining a **configuration** that specifies the training parameters, such as model architecture, optimizer settings, and training duration.\n",
    "\n",
    "Next, we‚Äôll instantiate a **trainer** using this configuration. The trainer will consume the dataset we previously created and manage the entire training process. By calling the `fit()` method on the trainer, we initiate a training job‚Äîwhose progress and status can be monitored throughout execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb420c88-6598-4641-b65b-5dacac65f494",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "# The first step is to define an instance of ExperimentConfig\n",
    "# ExperimentConfig points to the database and schema that we will\n",
    "# use to store model and experiment metadata such as metrics for\n",
    "# an experiment. This information will be visible from Snowflake\n",
    "# experiment tracking.\n",
    "experiment_config = ExperimentConfig(\n",
    "    database = DB_NAME,\n",
    "    schema = MODEL_REGISTRY_SCHEMA\n",
    ")\n",
    "\n",
    "\n",
    "# The second step will be to define a configuration for our Trainer.\n",
    "# The configuration includes many parameters that are explained in\n",
    "# detail in the documentation. It does not only provide parameters\n",
    "# for the graph neural network but also parameters for other components\n",
    "# of the model, such as feature extractors, prediction head parameters,\n",
    "# training parameters etc.\n",
    "model_config = TrainerConfig(\n",
    "    connector=connector,\n",
    "    experiment_config = experiment_config,\n",
    "    device=MODEL_DEVICE,  # either 'cuda' or 'cpu'\n",
    "    n_epochs=MODEL_N_EPOCHS,\n",
    "    max_iters=MODEL_MAX_ITERS,\n",
    "    text_embedder=MODEL_TEXT_EMBEDDER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d636f0-3f91-49f1-98e9-d70a61a9a1f2",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# we initialize now our trainer object with the trainer configuration\n",
    "# the trainer object can be used to train a model, to perform inference\n",
    "# or to perform training & inference.\n",
    "trainer = Trainer(connector=connector, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113a2d5-bcef-408b-8c09-3b3151edd8af",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "# in our first example we will use the trainer to perform training only.\n",
    "# every time the trainer is \"executed\" (calling fit(), predict() or fit_predict())\n",
    "# it returns a job object that can be used to monitor the current running job.\n",
    "# See the documentation for the meaning of the job statuses\n",
    "train_job = trainer.fit(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468d2a2-190b-4801-8bc8-66947d20572e",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "# we can also stream the logs of the training job in real time\n",
    "# Hint: You can stop the cell execution to stop monitoring of logs\n",
    "# Hint: At this point you can also open the MLFLow URL to monitor your experiments\n",
    "\n",
    "# Training will take approximately 6 to 7 minutes to complete.\n",
    "train_job.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6dd96-d809-4c87-81e5-a96bfcb733fa",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# hint: one can cancel a running job as well\n",
    "# train_job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa3a9c-e3ab-4edc-a9b9-29c627ad8e14",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "# now we can monitor the job status\n",
    "# observe that once the job is running we also get back an experiment name\n",
    "# we will see later how we can use that to perform inference\n",
    "train_job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the job is finished we can also register a model\n",
    "registered_model_name = \"hm_churn_model\"\n",
    "model_version = \"v1\"\n",
    "\n",
    "train_job.register_model(\n",
    "    model_name = registered_model_name, \n",
    "    version_name = model_version, \n",
    "    database_name = DB_NAME, \n",
    "    schema_name = MODEL_REGISTRY_SCHEMA\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736b155-cc8a-4fea-8a46-b63766a92f4e",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "## üîç Inference Using a Trained Model\n",
    "\n",
    "Finally, we‚Äôll demonstrate how to perform inference using the model we‚Äôve just trained. In this example, we'll directly use the recently trained model to generate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa944daa-898b-4771-a457-e7d575190790",
   "metadata": {
    "collapsed": false,
    "name": "cell47"
   },
   "source": [
    "‚ö†Ô∏è **Warning:** Existing Snowflake tables are never overwritten. To run inference and save predictions to a Snowflake table, you must either:\n",
    "\n",
    "* Change the OUTPUT_TABLE to a new, non-existent table, or\n",
    "\n",
    "* Delete the existing OUTPUT_TABLE (if you have the necessary permissions). You can do this by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce42c16-bf21-45e4-ab5f-30012acd56bb",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "# Note: We never overwrite already existing snowflake tables.\n",
    "# remove the previous predictions, if the table exists\n",
    "df = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\"); \n",
    "if (len(df.collect()) > 0): # table exists\n",
    "    df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_NAME}.PUBLIC.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n",
    "    df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_NAME}.PUBLIC.{OUTPUT_TABLE};\") ; df.collect()\n",
    "df = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE_EMB}';\"); \n",
    "if (len(df.collect()) > 0): # table exists\n",
    "    df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_NAME}.PUBLIC.{OUTPUT_TABLE_EMB} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n",
    "    df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_NAME}.PUBLIC.{OUTPUT_TABLE_EMB};\") ; df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cb76c-9fa5-470a-a432-fbb161c36cd2",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n",
    "# make sure that the table with the same alias does not already exist\n",
    "# we never overwrite tables\n",
    "\n",
    "inference_job = trainer.predict(\n",
    "    output_alias=OUTPUT_ALIAS,\n",
    "    output_config=output_config,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    dataset=dataset,\n",
    "    model_run_id=train_job.model_run_id,\n",
    "    extract_embeddings=True,\n",
    "    materialize_results=False\n",
    ")\n",
    "\n",
    "# Inference will take approximately 5 minutes to complete.\n",
    "inference_job.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a0b9c-b056-4795-a060-ee87a1fba517",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "inference_job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ea4fb-17dd-4a4f-a1e0-015c5973fb76",
   "metadata": {
    "language": "python",
    "name": "cell50"
   },
   "outputs": [],
   "source": [
    "inference_job.materialize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2762a5-0e31-45d1-8235-5789994c769c",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "# Finally, let's take a look at some of the predictions done by our GNN model\n",
    "# These predictions are saved in the OUTPUT_TABLE \n",
    "\n",
    "df = provider._session.sql(f\"SELECT * FROM {OUTPUT_TABLE} LIMIT 100;\") ; df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966d71b8-333c-4444-8c01-2dd4c4a88ad8",
   "metadata": {
    "collapsed": false,
    "name": "cell37"
   },
   "source": [
    "## üìã Job Manager\n",
    "\n",
    "It might be the case that we have lost track of the jobs that we are running. To this end we also provide to the user a JobManager object that can give us the status of all jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3531da-5454-455f-b80a-daf8743174d4",
   "metadata": {
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "# Let's see an example:\n",
    "job_manager = JobManager(connector=connector)\n",
    "job_manager.show_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b14b40-b477-429c-bd4f-a7e817be75b5",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "# You can retrieve job details using its job ID.\n",
    "# For example, use the job ID of the training job \n",
    "# you see above to access its details\n",
    "# and use the trained model for inference.\n",
    "\n",
    "# NOTE: To run this cell, replace the job ID below with the \n",
    "# actual ID from your training job shown in the previous cell.\n",
    "\n",
    "# hint: the job manager can be used to cancel any job as well\n",
    "\n",
    "# retrieved_job = job_manager.fetch_job(\"USE YOUR JOB ID HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bfd1d-3f83-44fd-83c8-4fc3fe6a9b9d",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "# Note: We never overwrite already existing snowflake tables.\n",
    "# remove the previous predictions, if the table exists\n",
    "\n",
    "# df = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\"); \n",
    "# if (len(df.collect()) > 0): # table exists\n",
    "#     df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_NAME}.PUBLIC.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n",
    "#     df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_NAME}.PUBLIC.{OUTPUT_TABLE};\") ; df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e3b4e-9de2-4757-8709-625f7ea9631c",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "# output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n",
    "\n",
    "# make sure that the table with the same alias does not already exist\n",
    "# we never overwrite tables\n",
    "# use the retrieved job to get the trained model for inference\n",
    "\n",
    "# inference_job_2 = trainer.predict(\n",
    "#     output_alias=OUTPUT_ALIAS,\n",
    "#     output_config=output_config,\n",
    "#     test_batch_size=TEST_BATCH_SIZE,\n",
    "#     dataset=dataset,\n",
    "#     model_run_id=retrieved_job.model_run_id,\n",
    "#     extract_embeddings=False,\n",
    "# )\n",
    "\n",
    "# inference_job_2.stream_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e7839-e76f-41f2-8018-19a929651a60",
   "metadata": {
    "collapsed": false,
    "name": "cell42"
   },
   "source": [
    "## üîç Inference With a Registered Model\n",
    "\n",
    "We can always do inference with a model that we have already registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91373e11-c101-4ab7-b81e-46c75a4f9dec",
   "metadata": {
    "language": "python",
    "name": "cell43"
   },
   "outputs": [],
   "source": [
    "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n",
    "\n",
    "# we can retrieve the registered model using:\n",
    "# the database that we have registered the model\n",
    "# the schema that we have registered the model\n",
    "# the registered model name\n",
    "# and the registered model version\n",
    "registered_model_signature = f\"{DB_NAME}.{MODEL_REGISTRY_SCHEMA}.{registered_model_name}.{model_version}\"\n",
    "\n",
    "# Hint: make sure that the table with the same alias does not already exist\n",
    "# we never overwrite tables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inference_job_reg = trainer.predict(\n",
    "    dataset=dataset,\n",
    "    registered_model_key=registered_model_name,\n",
    "    output_config=output_config,\n",
    "    output_alias=f\"{OUTPUT_ALIAS}_REG\",\n",
    "    extract_embeddings=False,\n",
    "    materialize_results=False\n",
    ")\n",
    "\n",
    "inference_job_reg.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d3fca-c5c6-4875-bd15-ff6aef0a4cd8",
   "metadata": {
    "language": "python",
    "name": "cell51"
   },
   "outputs": [],
   "source": [
    "inference_job_reg.materialize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bc60a-bd46-4321-a064-489e7ffe5836",
   "metadata": {
    "language": "python",
    "name": "cell48"
   },
   "outputs": [],
   "source": [
    "inference_job_reg.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b1ea2-92ea-46f4-99dc-f6de44461b6f",
   "metadata": {
    "collapsed": false,
    "name": "cell44"
   },
   "source": [
    "## üóÇÔ∏è Model Manager\n",
    "\n",
    "The model manager class helps us list trained and registered models, regiter a model or delete trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca7856-60fb-44a5-a4fc-7165bbb99bae",
   "metadata": {
    "language": "python",
    "name": "cell49"
   },
   "outputs": [],
   "source": [
    "# initialize the model manager\n",
    "model_manager = ModelManager(connector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a model\n",
    "\n",
    "# The model is registered under the specified name and version in \n",
    "# the database and schema that snowflake experiment tracker has permissions to. \n",
    "# Registration is only allowed if a valid `model_run_id` exists. \n",
    "# Duplicate version names for a model are not allowed. \n",
    "model_version_2 = \"v2\"\n",
    "model_manager.register_model(\n",
    "    model_run_id=train_job.model_run_id,\n",
    "    experiment_name=train_job.experiment_name,\n",
    "    model_name=registered_model_name,\n",
    "    version_name=model_version_2,\n",
    "    database_name=DB_NAME,\n",
    "    schema_name=MODEL_REGISTRY_SCHEMA,\n",
    "    comment=\"Production-ready model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list trained and registered models\n",
    "model_manager.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permanently deletes a trained model from the relational AI application stage.\n",
    "# Does not delete a registered model\n",
    "model_manager.delete_model(\n",
    "    experiment_name =train_job.experiment_name,\n",
    "    model_run_id = train_job.model_run_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai_gnn_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "lastEditStatus": {
   "authorEmail": "dafni.anagnostopoulou@relational.ai",
   "authorId": "5493200917234",
   "authorName": "dafni.anagnostopoulou@relational.ai",
   "lastEditTime": 1755598056631,
   "notebookId": "fpsmbzz4d2kpuh3lqf5p",
   "sessionId": "dc573eca-308f-464e-a1de-da5cf1ec0f4a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

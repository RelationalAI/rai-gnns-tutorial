{
 "metadata": {
  "kernelspec": {
   "display_name": "sdk_ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "lastEditStatus": {
   "notebookId": "wiue4ajxjxivq5qomwyk",
   "authorId": "5493200917234",
   "authorName": "dafni.anagnostopoulou@relational.ai",
   "authorEmail": "dafni.anagnostopoulou@relational.ai",
   "sessionId": "653aadfe-905b-4fca-9761-9477c052db76",
   "lastEditTime": 1752833495876
  }
 },
 "nbformat_minor": 2,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f83d5ea-0dcc-45a9-8d32-57a393a0822b",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# HM Purchase Recommendations: Link Prediction example using the GNN learning engine under the RelationalAI Snowflake Native App"
  },
  {
   "cell_type": "markdown",
   "id": "fab2e147-9fc1-4fc6-a0ca-815ea464d56d",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "### ðŸ” Connecting to the RelationalAI Native App via Snowflake\n\nTo connect to the RelationalAI native app, we'll first set up a dictionary that defines the necessary environment variables for establishing a Snowflake connection. Here we will use\nthe `active_session` authentication method which gets all the credentials needed from the\ncurrent snowflake session. We only need to provide the application name (`RAI_EXPT_APP`).\n\n#### ðŸ” About Authentication\n\nIn this tutorial, weâ€™ll use **active_session**. However, other authentication methodsâ€”such as **Key Pair Authentication**, **Password** or **OAuth Token Authentication**â€”are also supported. \n"
  },
  {
   "cell_type": "code",
   "id": "8a7f415d-2628-4867-93ef-fef1bdf67760",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "# load the snowflake configuration to a python dict\nsnowflake_config = {\n    \"app_name\": \"RELATIONALAI\",\n    \"auth_method\": \"active_session\"\n}",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6375d429-69bb-413a-be73-f4db947d3510",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## âš™ï¸ Managing Your GNN Engines\n\nThe `Provider` class in the RelationalAI GNN SDK allows you to manage your GNN engines seamlessly. Below, we walk through common operations you can perform with the `Provider`:\n\n* âœ… Create a new GNN engine\n* ðŸ“‹ List all available engines\n* ðŸ” Check the status of an engine\n* ðŸ”„ Resume a paused engine\n* âŒ Delete an engine\n\nEach of these operations can be done with simple method calls, as shown in the following examples."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "name": "cell5",
    "language": "python"
   },
   "outputs": [],
   "source": "from rai_gnns_experimental import GNNTable, ForeignKey\nfrom rai_gnns_experimental import ColumnDType\nfrom rai_gnns_experimental import EvaluationMetric\nfrom rai_gnns_experimental import LinkTask, TaskType\nfrom rai_gnns_experimental import Dataset\nfrom rai_gnns_experimental import TrainerConfig\nfrom rai_gnns_experimental import Trainer\nfrom rai_gnns_experimental import JobManager\nfrom rai_gnns_experimental import OutputConfig, SnowflakeConnector, Provider\n\nfrom graphviz import Source\nimport time",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "code",
   "id": "02ab23bb-f651-47c8-b1ee-dc642cf4bdfb",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "# engine setup\nengine_name = \"my_engine\"\nengine_size = \"GPU_NV_S\" # Available sizes: \"GPU_NV_S\" or \"HIGHMEM_X64_S\"\n\n# database /s chema\nDB_NAME = \"HM_DB\"\nDB_SCHEMA_NAME = \"HM_SCHEMA\"\nTASK_SCHEMA_NAME = \"HM_PURCHASE\"\nDB_SCHEMA = f\"{DB_NAME}.{DB_SCHEMA_NAME}\"\nTASK_SCHEMA = f\"{DB_NAME}.{TASK_SCHEMA_NAME}\"\n\n# dataset name\nDATASET_NAME = \"hm_purchase_dataset\"\n\n# customer data\nCUSTOMER_SOURCE_TABLE = f\"{DB_SCHEMA}.CUSTOMERS\"\nCUSTOMER_NAME = \"customers\"\nCUSTOMER_PRIMARY_KEY = \"customer_id\"\n\n# article data\nARTICLE_SOURCE_TABLE = f\"{DB_SCHEMA}.ARTICLES\"\nARTICLE_NAME = \"articles\"\nARTICLE_PRIMARY_KEY = \"article_id\"\n\n# transaction data\nTRANSACTION_SOURCE_TABLE = f\"{DB_SCHEMA}.TRANSACTIONS\"\nTRANSACTION_NAME = \"transactions\"\nTIME_COLUMN = \"t_dat\"\n\n# node task\nLINK_TASK_NAME = \"purchase_task\"\nTASK_TIME_COLUMN_NAME = \"timestamp\"\nTASK_TRAIN_TABLE = f\"{TASK_SCHEMA}.TRAIN\"\nTASK_TEST_TABLE  = f\"{TASK_SCHEMA}.TEST\"\nTASK_VALIDATION_TABLE = f\"{TASK_SCHEMA}.VALIDATION\"\n\n# model params\nMODEL_DEVICE = \"cuda\" # either 'cuda' or 'cpu'\nMODEL_N_EPOCHS = 3\nMODEL_MAX_ITERS = 200\nMODEL_TEXT_EMBEDDER = \"model2vec-potion-base-4M\"\n\n\nOUTPUT_ALIAS = \"PURCHASE_TEST_PREDS_1\"\nOUTPUT_TABLE = f\"PREDICTIONS_{OUTPUT_ALIAS}\"\nTEST_BATCH_SIZE = 128",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d630bc70-9f41-4793-8926-b9c82bf6f86e",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# initialize the provider using the snowflake configuration\n# (note: you might be prompted from your MFA app at this point)\nprovider = Provider(**snowflake_config)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8625eaca-8ff4-450b-ba76-268acb56c60d",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "# Create a new GNN engine.\n# Currently supported engine types:\n# - For Snowflake accounts hosted on AWS: \"GPU_NV_S\" and \"HIGHMEM_X64_S\"\n# - For Snowflake accounts hosted on Azure: \"HIGHMEM_X64_S\" only\n#\n# You must provide:\n# - A custom engine name (via the `name` parameter)\n# - The engine type (via the `size` parameter)\n#\n# Available sizes:\n# - AWS: \"GPU_NV_S\", \"HIGHMEM_X64_S\"\n# - Azure: \"HIGHMEM_X64_S\"\n#\n# provider.create_gnn(\n#    name=engine_name,\n#    size=\"GPU_NV_S\"  # or \"HIGHMEM_X64_S\"\n#)\n\n\n# The creation of a new engine may take \n# several minutes to complete (e.g., ~4 minutes).\n# Be patient and do not interrupt the process.\n\n\n# check if engine exists, if yes, resume if not create it\nif not provider.get_gnn(engine_name): \n    print(f\"Creating Engine {engine_name}\")\n    provider.create_gnn(name=engine_name, size=engine_size)\nelse:\n    print(f\"Resuming Engine {engine_name}\")\n    provider.resume_gnn(name=engine_name)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "849a8abe-cb3e-4e37-9119-c4489e062585",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "# HINT: We can always resume a GNN engine that has been suspended:\n# Note: Engine provisioning can take some minutes. Please\n# check the engine status using provider.get_gnn(name=\"my_engine\")\n\n# provider.resume_gnn(name=engine_name)\n\n# And if we need we can also delete a GNN engine\n\n# provider.delete_gnn(name=engine_name)\n\n# we can also check the existing engines. If there is not engines listed here you would need to create one using provider.create_gnn(name=\"my_engine\", size=\"GPU_NV_S\")\n# provider.list_gnns()\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c9757278-6ec0-46f4-9143-772753854441",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# If we resume an engine, we can directly see the status of the engine\n# the status of the engine 'READY' marks the fact that the\n# engine is ready to be used. A `PENDING` status marks the\n# fact that the engine has  been automaticaly suspended.\n# Notice also that under the settings \n# the provider exposes a URL for the MLFLOW endpoint\n# that we can use to track our experiments\n# NOTE: we should wait until the status is READY\n\nengine_data = provider.get_gnn(engine_name)\nif engine_data[\"state\"] == 'SUSPENDED':\n    print(f'ENGINE {engine_name} IS SUSPENDED, YOU HAVE TO RESUME IT FIRST')\nelse:\n    while not engine_data or engine_data[\"state\"] != 'READY':\n        time.sleep(10)\n        engine_data = provider.get_gnn(engine_name) \n    \n\n    print(f'ENGINE {engine_name} READY')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b14e267a-1d32-4652-9e2c-b548cbdfc3eb",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "âš ï¸ **Warning:** To  make sure to check the engine status (e.g., via `provider.get_gnn(\"engine_name\")`) and make sure that the status is `READY` "
  },
  {
   "cell_type": "markdown",
   "id": "271218c4-729d-490d-87cf-c77775989e39",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## ðŸ”Œ Connector Setup\n\nThe `Connector` class, like the `Provider` class, is used to communicate with Snowflake. However, while the `Provider` is responsible for managing GNN engines, the `Connector` is specifically used to interface with the **GNN learning engine** itself.\n\nYouâ€™ll use the `Connector` instance as an input to all SDK components that need to send requests to the GNN engineâ€”such as loading data, running training jobs, or performing inference.\n\nIn short:\n\n* `Provider` â†’ Manages GNN engine instances (create, list, delete, etc.)\n* `Connector` â†’ Sends requests to a specific GNN engine for processing tasks\n\nLetâ€™s now walk through how to create and use a `Connector`.\n"
  },
  {
   "cell_type": "code",
   "id": "6112385d-1f33-48c7-893d-1ef7debc85e6",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# we initialize the connector and passing all our credentials.\nconnector = SnowflakeConnector(\n    **snowflake_config,\n    engine_name=engine_name,\n)\n# the connector also provides access to MLFLOW that you can\n# use to monitor your experiments and register trained GNN models\n# connector.mlflow_session_url",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d3cc3c2-41b4-44c2-85d9-d325d7f6f43c",
   "metadata": {
    "name": "cell37",
    "collapsed": false
   },
   "source": "## ðŸ“ˆ MLflow: Monitor Training\n\nYou can visit MLflow to monitor the training process in real time, including loss trends and evaluation metrics, using the mlflowendpoint ingress url. For a detailed example on how to use MLflow you can visit https://github.com/RelationalAI/rai-gnns-tutorial/blob/main/HM/MLflow.md "
  },
  {
   "cell_type": "code",
   "id": "a79027c7-4fe8-4248-abc2-37504c2e06c0",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "connector.get_gnn(engine_name)[\"settings\"][\"mlflowendpoint\"]",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9cfdba1-1c11-483a-a05b-2f81d546b79e",
   "metadata": {
    "name": "cell14",
    "collapsed": false
   },
   "source": "## ðŸ“Š Preparing the data: Creating the GNN tables\n\nIn this section, we will define the GNN tables and the associated learning task. These components will then be used to construct a GNN dataset suitable for training.\n\nFor this tutorial, weâ€™ll use the H&M database as our working example. This database includes three tables: CUSTOMERS, ARTICLES, and TRANSACTIONS. The TRANSACTIONS table links CUSTOMERS to ARTICLES. Our objective is to predict if a given CUSTOMER is going to churn in the next week, meaning if they are going to stop making any TRANSACTIONS. We handle this problem as a binary node classification task (churn=1/no churn=0).\n"
  },
  {
   "cell_type": "code",
   "id": "fa9cd838-ded1-4c2d-aa36-007aea26767b",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "# create a table for the customers and set the \n# customer_id as a primary key (primary and \n# foreign keys are used to construct the edges of the graph)\ncustomers_table = GNNTable(\n    connector=connector,\n    source=CUSTOMER_SOURCE_TABLE,\n    name=CUSTOMER_NAME,\n    primary_key=CUSTOMER_PRIMARY_KEY,\n)\ncustomers_table.show_table()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4de06867-2433-44db-a532-76dd5044fd5f",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "# in a similar manner we can create the ARTICLES table\narticles_table = GNNTable(\n    connector=connector,\n    source=ARTICLE_SOURCE_TABLE,\n    name=ARTICLE_NAME,\n    primary_key=ARTICLE_PRIMARY_KEY,\n)\narticles_table.show_table()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e6f426a-0063-4e14-aa5e-f5e34e91cb0b",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": "# and finally we will link the two tables using the foreign\n# keys from the TRANSACTIONS table. Note: the transactions\n# table  has also one special \"time column\" that will be used\n# to prevent data leakage (see the documentation for more details)\ntransactions_table = GNNTable(\n    connector=connector,\n    source=TRANSACTION_SOURCE_TABLE,\n    name=TRANSACTION_NAME,\n    foreign_keys=[\n        ForeignKey(\n            column_name=CUSTOMER_PRIMARY_KEY, link_to=CUSTOMER_NAME+\".\"+CUSTOMER_PRIMARY_KEY),\n        ForeignKey(\n            column_name=ARTICLE_PRIMARY_KEY, link_to=ARTICLE_NAME+\".\"+ARTICLE_PRIMARY_KEY),\n    ],\n    time_column=TIME_COLUMN,\n)\ntransactions_table.show_table()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1961b27-cac3-441d-90fa-b0ae1495faff",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "\n## ðŸ”§ Preparing the Data: Creating the Task\n\nTo define the task, we begin by specifying the locations of the training, validation, and test datasets. We also identify the source and destination entity tables, along with the corresponding columns that uniquely identify each entity.\n\nSince this is a **link prediction** task, our objective is to predict future connections between a source entity and a destination entity.\n\nAdditionally, we define a **timestamp column** to avoid information leakage by ensuring that future data doesn't influence past predictions. Lastly, we specify the **evaluation metric**â€”in this case, **Mean Average Precision (MAP)**â€”to assess the performance of the model.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell19",
    "language": "python"
   },
   "outputs": [],
   "source": "link_pred_task = LinkTask(\n    connector=connector,\n    name=LINK_TASK_NAME,\n    task_data_source={\n        \"train\": TASK_TRAIN_TABLE, \n        \"test\": TASK_TEST_TABLE, \n        \"validation\": TASK_VALIDATION_TABLE\n    },\n    # name of source entity column that we want to do predictions for\n    source_entity_column=CUSTOMER_PRIMARY_KEY,\n    # name of GNN table that column is at\n    source_entity_table=CUSTOMER_NAME,\n    # name of target entity column that we want to predict\n    target_entity_column=ARTICLE_PRIMARY_KEY,\n    # name of GNN table that column is at\n    target_entity_table=ARTICLE_NAME,\n    time_column=TASK_TIME_COLUMN_NAME,\n    task_type=TaskType.LINK_PREDICTION,\n    evaluation_metric=EvaluationMetric(name=\"link_prediction_map\", eval_at_k=12),\n)\n\nlink_pred_task.show_task()",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "id": "abdfe70e-475a-4fe7-8260-ce08b43ec297",
   "metadata": {
    "name": "cell20",
    "collapsed": false
   },
   "source": "## ðŸ§© Preparing the Data: Creating the Dataset\n\nFinally, we combine all the components by constructing a dataset object that encapsulates both the GNN tables and the task definition. This dataset will serve as the input to the model training pipeline, ensuring that the task and its associated data are tightly integrated and ready for downstream processing.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": "dataset = Dataset(\n    connector=connector,\n    dataset_name=DATASET_NAME,\n    tables=[articles_table, customers_table, transactions_table],\n    task_description=link_pred_task,\n)",
   "id": "ce110000-1111-2222-3333-ffffff000013"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell22",
    "language": "python"
   },
   "outputs": [],
   "source": "# we can also visualize the dataset \ngraph = dataset.visualize_dataset(show_dtypes=True)\n# play with font size and plot size to get a good visualization\nfor node in graph.get_nodes():    \n    font_size = node.get_attributes()['fontsize']\n    font_size = \"16\"\n    node.set('fontsize', font_size)\n\ngraph.set_graph_defaults(size=\"10,10!\")  # Increase graph size\n\nsrc = Source(graph.to_string())\nsrc  # Display in notebo",
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "id": "893e1161-f0f7-4d1a-83fe-07cb7c340562",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "## ðŸš€ GNN Model Training\n\nNow that our dataset is ready, we can train our first GNN model. Weâ€™ll begin by defining a **configuration** that specifies the training parameters, such as model architecture, optimizer settings, and training duration.\n\nNext, weâ€™ll instantiate a **trainer** using this configuration. The trainer will consume the dataset we previously created and manage the entire training process. By calling the `fit()` method on the trainer, we initiate a training jobâ€”whose progress and status can be monitored throughout execution."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "cell24",
    "language": "python"
   },
   "outputs": [],
   "source": "# the first step will be to define a configuration for our Trainer.\n# the configuration includes many parameters that are explained in\n# detail in the documentation. It does not only provide parameters\n# for the graph neural network but also parameters for other components\n# of the model (such as feature extractors, prediction head parameters,\n# training parameters etc.)\nmodel_config = TrainerConfig(\n    connector=connector,\n    device=MODEL_DEVICE,  # either 'cuda' or 'cpu'\n    n_epochs=MODEL_N_EPOCHS,\n    max_iters=MODEL_MAX_ITERS,\n    text_embedder=MODEL_TEXT_EMBEDDER,\n)",
   "id": "ce110000-1111-2222-3333-ffffff000016"
  },
  {
   "cell_type": "code",
   "id": "833f9f2a-4036-4ae0-bfa9-8c96682cdb59",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": "# we initialize now our trainer object with the trainer configuration\n# the trainer object can be used to train a model, to perform inference\n# or to perform training & inference.\ntrainer = Trainer(connector=connector, config=model_config)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b87f2d2f-eba9-4282-bc84-7731093ced4b",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "# in our first example we will use the trainer to perform training only.\n# every time the trainer is \"executed\" (calling fit(), predict() or fit_predict())\n# it returns a job object that can be used to monitor the current running job.\n# See the documentation for the meaning of the job statuses\ntrain_job = trainer.fit(dataset=dataset)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c43c0738-e489-4d97-8b01-04bc1fff4b22",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "# we can also stream the logs of the training job in real time\n# Hint: You can stop the cell execution to stop monitoring of logs\n# Hint: At this point you can also open the MLFLow URL to monitor your experiments\ntrain_job.stream_logs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b78ad33b-5eee-484c-a5a0-29d76cd3c088",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "# hint: one can cancel a running job as well\n# train_job.cancel()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "608d2d96-9185-45f5-9efc-19e4f8fa6873",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": "# now we can monitor the job status\n# observe that once the job is running we also get back an experiment name\n# we will see later how we can use that to perform inference\ntrain_job.get_status()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "18a015d1-8eec-4d76-b409-0b2eaef2f897",
   "metadata": {
    "name": "cell32",
    "collapsed": false
   },
   "source": "## ðŸ” Inference Using a Trained Model\n\nFinally, weâ€™ll demonstrate how to perform inference using the model weâ€™ve just trained. In this example, we'll directly use the recently trained model to generate predictions.\n\nFor more advanced use casesâ€”such as registering a model for reuse or automatically selecting the best-performing modelâ€”please refer to the churn prediction notebook.\n"
  },
  {
   "cell_type": "markdown",
   "id": "85c715ba-1895-490b-9d4a-64fe9c947931",
   "metadata": {
    "name": "cell42",
    "collapsed": false
   },
   "source": "âš ï¸ **Warning:** Existing Snowflake tables are never overwritten. To run inference and save predictions to a Snowflake table, you must either:\n\n* Change the OUTPUT_TABLE to a new, non-existent table, or\n\n* Delete the existing OUTPUT_TABLE (if you have the necessary permissions). You can do this by running the cell below."
  },
  {
   "cell_type": "code",
   "id": "fd87c0eb-c76e-486e-9a52-346762b2993f",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": "# remove the previous predictions, if the table exists\ndf = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\"); \nif (len(df.collect()) > 0): # table exists\n    df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_SCHEMA}.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n    df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_SCHEMA}.{OUTPUT_TABLE};\") ; df.collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "593de278-2e2a-4421-8726-eca08c4c822e",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n# make sure that the table with the same alias does not already exist\n# we never overwrite tables\n\ninference_job = trainer.predict(\n    output_alias=OUTPUT_ALIAS,\n    output_config=output_config,\n    test_batch_size=TEST_BATCH_SIZE,\n    dataset=dataset,\n    model_run_id=train_job.model_run_id,\n    extract_embeddings=True,\n)\n\ninference_job.stream_logs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb9bb867-680d-486f-8060-0a197036618d",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "inference_job.get_status()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "372221b2-2fe8-4804-99b0-bf8bfb70c5da",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "# Finally, let's take a look at some of the predictions done by our GNN model\n# These predictions are saved in the OUTPUT_TABLE \n\ndf = provider._session.sql(f\"SELECT * FROM {OUTPUT_TABLE} LIMIT 100;\") ; df.collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b754be8-7004-4216-b85d-a00358573ed3",
   "metadata": {
    "name": "cell39",
    "collapsed": false
   },
   "source": "## ðŸ“‹ Job Manager\n\nIt might be the case that we have lost track of the jobs that we are running. To this end we also provide to the user a JobManager object that can give us the status of all jobs."
  },
  {
   "cell_type": "code",
   "id": "2a364f3c-0ed1-4f03-8ec9-349d93100dd3",
   "metadata": {
    "language": "python",
    "name": "cell30",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's see an example:\njob_manager = JobManager(connector=connector)\njob_manager.show_jobs()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5fcbeb6-55ea-4242-a343-e31accac11e5",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": "# You can retrieve job details using its job ID.\n# For example, use the job ID of the training job above to access its details\n# and use the trained model for inference.\n\n# NOTE: To run this cell, replace the job ID below with the actual ID\n# from your training job output in the previous cell.\n\nretrieved_job = job_manager.fetch_job(\"01bdaf68-020e-076c-000a-1dc701bcbba6\")\n# hint: the job manager can be used to cancel any job as well",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21a96893-faf2-4eab-97bd-e6d7e3aaf58c",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": "# remove the previous predictions, if the table exists\ndf = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\"); \nif (len(df.collect()) > 0): # table exists\n    df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_SCHEMA}.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n    df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_SCHEMA}.{OUTPUT_TABLE};\") ; df.collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb98566e-19f4-4634-92b0-90edc0a2e4a3",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n# make sure that the table with the same alias does not already exist\n# we never overwrite tables\n# use the retrieved job to get the trained model for inference\n\ninference_job = trainer.predict(\n    output_alias=OUTPUT_ALIAS,\n    output_config=output_config,\n    test_batch_size=TEST_BATCH_SIZE,\n    dataset=dataset,\n    model_run_id=retrieved_job.model_run_id,\n    extract_embeddings=False,\n)\n\ninference_job.stream_logs()",
   "execution_count": null
  }
 ]
}
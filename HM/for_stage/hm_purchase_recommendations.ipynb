{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f83d5ea-0dcc-45a9-8d32-57a393a0822b",
   "metadata": {
    "collapsed": false,
    "name": "cell1"
   },
   "source": [
    "# HM Purchase Recommendations: Link Prediction example using the GNN learning engine under the RelationalAI Snowflake Native App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab2e147-9fc1-4fc6-a0ca-815ea464d56d",
   "metadata": {
    "collapsed": false,
    "name": "cell2"
   },
   "source": [
    "### ðŸ” Connecting to the RelationalAI Native App via Snowflake\n",
    "\n",
    "To connect to the RelationalAI native app, we'll first set up a dictionary that defines the necessary environment variables for establishing a Snowflake connection. Here we will use\n",
    "the `active_session` authentication method which gets all the credentials needed from the\n",
    "current snowflake session. We only need to provide the application name (`RELATIONALAI`).\n",
    "\n",
    "#### ðŸ” About Authentication\n",
    "\n",
    "In this tutorial, weâ€™ll use **active_session**. However, other authentication methodsâ€”such as **Key Pair Authentication**, **Password** or **OAuth Token Authentication**â€”are also supported. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f415d-2628-4867-93ef-fef1bdf67760",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": [
    "# load the snowflake configuration to a python dict\n",
    "snowflake_config = {\n",
    "    \"app_name\": \"RELATIONALAI\",\n",
    "    \"auth_method\": \"active_session\",\n",
    "    \"role\": \"SYSADMIN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375d429-69bb-413a-be73-f4db947d3510",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "## âš™ï¸ Managing Your GNN Engines\n",
    "\n",
    "The `Provider` class in the RelationalAI GNN SDK allows you to manage your GNN engines seamlessly. Below, we walk through common operations you can perform with the `Provider`:\n",
    "\n",
    "* âœ… Create a new GNN engine\n",
    "* ðŸ“‹ List all available engines\n",
    "* ðŸ” Check the status of an engine\n",
    "* ðŸ”„ Resume a paused engine\n",
    "* âŒ Delete an engine\n",
    "\n",
    "Each of these operations can be done with simple method calls, as shown in the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": [
    "from rai_gnns_experimental import GNNTable, ForeignKey, CandidateKey\n",
    "from rai_gnns_experimental import ColumnDType\n",
    "from rai_gnns_experimental import EvaluationMetric\n",
    "from rai_gnns_experimental import LinkTask, TaskType\n",
    "from rai_gnns_experimental import Dataset\n",
    "from rai_gnns_experimental import TrainerConfig\n",
    "from rai_gnns_experimental import Trainer\n",
    "from rai_gnns_experimental import JobManager\n",
    "from rai_gnns_experimental import OutputConfig, SnowflakeConnector, Provider\n",
    "\n",
    "from graphviz import Source\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab23bb-f651-47c8-b1ee-dc642cf4bdfb",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "# engine setup\n",
    "engine_name = \"my_engine\"\n",
    "engine_size = \"GPU_NV_S\" # Available sizes: \"GPU_NV_S\" or \"HIGHMEM_X64_S\"\n",
    "\n",
    "# database /s chema\n",
    "DB_NAME = \"HM_DB_EDGE_IF\"\n",
    "DB_SCHEMA_NAME = \"HM_SCHEMA\"\n",
    "TASK_SCHEMA_NAME = \"HM_PURCHASE\"\n",
    "DB_SCHEMA = f\"{DB_NAME}.{DB_SCHEMA_NAME}\"\n",
    "TASK_SCHEMA = f\"{DB_NAME}.{TASK_SCHEMA_NAME}\"\n",
    "\n",
    "# dataset name\n",
    "DATASET_NAME = \"hm_purchase_dataset\"\n",
    "\n",
    "# customer data\n",
    "CUSTOMER_SOURCE_TABLE = f\"{DB_SCHEMA}.CUSTOMERS\"\n",
    "CUSTOMER_NAME = \"customers\"\n",
    "CUSTOMER_CANDIDATE_KEY = \"customer_id\"\n",
    "\n",
    "# article data\n",
    "ARTICLE_SOURCE_TABLE = f\"{DB_SCHEMA}.ARTICLES\"\n",
    "ARTICLE_NAME = \"articles\"\n",
    "ARTICLE_CANDIDATE_KEY = \"article_id\"\n",
    "\n",
    "# transaction data\n",
    "TRANSACTION_SOURCE_TABLE = f\"{DB_SCHEMA}.TRANSACTIONS\"\n",
    "TRANSACTION_NAME = \"transactions\"\n",
    "TIME_COLUMN = \"t_dat\"\n",
    "\n",
    "# node task\n",
    "LINK_TASK_NAME = \"purchase_task\"\n",
    "TASK_TIME_COLUMN_NAME = \"TIMESTAMP\"\n",
    "TASK_TRAIN_TABLE = f\"{TASK_SCHEMA}.TRAIN\"\n",
    "TASK_TEST_TABLE  = f\"{TASK_SCHEMA}.TEST\"\n",
    "TASK_VALIDATION_TABLE = f\"{TASK_SCHEMA}.VALIDATION\"\n",
    "TASK_SOURCE_ENTITY_COLUMN_NAME = \"customer_id\"\n",
    "TASK_SOURCE_ENTITY_COLUMN_LINKTO = f\"{CUSTOMER_NAME}.{CUSTOMER_CANDIDATE_KEY}\"\n",
    "TASK_TARGET_ENTITY_COLUMN_NAME = \"article_id\"\n",
    "TASK_TARGET_ENTITY_COLUMN_LINKTO = f\"{ARTICLE_NAME}.{ARTICLE_CANDIDATE_KEY}\"\n",
    "\n",
    "# model params\n",
    "MODEL_DEVICE = \"cuda\" # either 'cuda' or 'cpu'\n",
    "MODEL_N_EPOCHS = 3\n",
    "MODEL_MAX_ITERS = 200\n",
    "MODEL_TEXT_EMBEDDER = \"model2vec-potion-base-4M\"\n",
    "\n",
    "\n",
    "OUTPUT_ALIAS = \"PURCHASE_TEST_PREDS_1\"\n",
    "OUTPUT_TABLE = f\"PREDICTIONS_{OUTPUT_ALIAS}\"\n",
    "TEST_BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630bc70-9f41-4793-8926-b9c82bf6f86e",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": [
    "# initialize the provider using the snowflake configuration\n",
    "# (note: you might be prompted from your MFA app at this point)\n",
    "provider = Provider(**snowflake_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625eaca-8ff4-450b-ba76-268acb56c60d",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "# Create a new GNN engine.\n",
    "# Currently supported engine types:\n",
    "# - For Snowflake accounts hosted on AWS: \"GPU_NV_S\" and \"HIGHMEM_X64_S\"\n",
    "# - For Snowflake accounts hosted on Azure: \"HIGHMEM_X64_S\" only\n",
    "#\n",
    "# You must provide:\n",
    "# - A custom engine name (via the `name` parameter)\n",
    "# - The engine type (via the `size` parameter)\n",
    "#\n",
    "# Available sizes:\n",
    "# - AWS: \"GPU_NV_S\", \"HIGHMEM_X64_S\"\n",
    "# - Azure: \"HIGHMEM_X64_S\"\n",
    "#\n",
    "# provider.create_gnn(\n",
    "#    name=engine_name,\n",
    "#    size=\"GPU_NV_S\"  # or \"HIGHMEM_X64_S\"\n",
    "#)\n",
    "\n",
    "\n",
    "# The creation of a new engine may take \n",
    "# several minutes to complete (e.g., ~4 minutes).\n",
    "# Be patient and do not interrupt the process.\n",
    "\n",
    "\n",
    "# check if engine exists, if yes, resume if not create it\n",
    "if not provider.get_gnn(engine_name): \n",
    "    print(f\"Creating Engine {engine_name}\")\n",
    "    provider.create_gnn(name=engine_name, size=engine_size)\n",
    "else:\n",
    "    print(f\"Resuming Engine {engine_name}\")\n",
    "    provider.resume_gnn(name=engine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a8abe-cb3e-4e37-9119-c4489e062585",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": [
    "# HINT: We can always resume a GNN engine that has been suspended:\n",
    "# Note: Engine provisioning can take some minutes. Please\n",
    "# check the engine status using provider.get_gnn(name=\"my_engine\")\n",
    "\n",
    "# provider.resume_gnn(name=engine_name)\n",
    "\n",
    "# And if we need we can also delete a GNN engine\n",
    "\n",
    "# provider.delete_gnn(name=engine_name)\n",
    "\n",
    "# we can also check the existing engines. If there is not engines listed here you would need to create one using provider.create_gnn(name=\"my_engine\", size=\"GPU_NV_S\")\n",
    "# provider.list_gnns()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9757278-6ec0-46f4-9143-772753854441",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": [
    "# If we resume an engine, we can directly see the status of the engine\n",
    "# the status of the engine 'READY' marks the fact that the\n",
    "# engine is ready to be used. A `PENDING` status marks the\n",
    "# fact that the engine has  been automaticaly suspended.\n",
    "# Notice also that under the settings \n",
    "# the provider exposes a URL for the MLFLOW endpoint\n",
    "# that we can use to track our experiments\n",
    "# NOTE: we should wait until the status is READY\n",
    "\n",
    "engine_data = provider.get_gnn(engine_name)\n",
    "if engine_data[\"state\"] == 'SUSPENDED':\n",
    "    print(f'ENGINE {engine_name} IS SUSPENDED, YOU HAVE TO RESUME IT FIRST')\n",
    "else:\n",
    "    while not engine_data or engine_data[\"state\"] != 'READY':\n",
    "        time.sleep(10)\n",
    "        engine_data = provider.get_gnn(engine_name) \n",
    "    \n",
    "\n",
    "    print(f'ENGINE {engine_name} READY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e267a-1d32-4652-9e2c-b548cbdfc3eb",
   "metadata": {
    "collapsed": false,
    "name": "cell11"
   },
   "source": [
    "âš ï¸ **Warning:** To  make sure to check the engine status (e.g., via `provider.get_gnn(\"engine_name\")`) and make sure that the status is `READY` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271218c4-729d-490d-87cf-c77775989e39",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "## ðŸ”Œ Connector Setup\n",
    "\n",
    "The `Connector` class, like the `Provider` class, is used to communicate with Snowflake. However, while the `Provider` is responsible for managing GNN engines, the `Connector` is specifically used to interface with the **GNN learning engine** itself.\n",
    "\n",
    "Youâ€™ll use the `Connector` instance as an input to all SDK components that need to send requests to the GNN engineâ€”such as loading data, running training jobs, or performing inference.\n",
    "\n",
    "In short:\n",
    "\n",
    "* `Provider` â†’ Manages GNN engine instances (create, list, delete, etc.)\n",
    "* `Connector` â†’ Sends requests to a specific GNN engine for processing tasks\n",
    "\n",
    "Letâ€™s now walk through how to create and use a `Connector`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112385d-1f33-48c7-893d-1ef7debc85e6",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "# we initialize the connector and passing all our credentials.\n",
    "connector = SnowflakeConnector(\n",
    "    **snowflake_config,\n",
    "    engine_name=engine_name,\n",
    ")\n",
    "# the connector also provides access to MLFLOW that you can\n",
    "# use to monitor your experiments and register trained GNN models\n",
    "# connector.mlflow_session_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3cc3c2-41b4-44c2-85d9-d325d7f6f43c",
   "metadata": {
    "collapsed": false,
    "name": "cell37"
   },
   "source": [
    "## ðŸ“ˆ MLflow: Monitor Training\n",
    "\n",
    "You can visit MLflow to monitor the training process in real time, including loss trends and evaluation metrics, using the mlflowendpoint ingress url. For a detailed example on how to use MLflow you can visit https://github.com/RelationalAI/rai-gnns-tutorial/blob/main/HM/MLflow.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79027c7-4fe8-4248-abc2-37504c2e06c0",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "connector.mlflow_session_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cfdba1-1c11-483a-a05b-2f81d546b79e",
   "metadata": {
    "collapsed": false,
    "name": "cell14"
   },
   "source": [
    "## ðŸ“Š Preparing the data: Creating the GNN tables\n",
    "\n",
    "In this section, we will define the GNN tables and the associated learning task. These components will then be used to construct a GNN dataset suitable for training.\n",
    "\n",
    "For this tutorial, weâ€™ll use the H&M database as our working example. This database includes three tables: CUSTOMERS, ARTICLES, and TRANSACTIONS. The TRANSACTIONS table links CUSTOMERS to ARTICLES. Our objective is to predict if a given CUSTOMER is going to predict the set of articles each customer will purchase in the next seven days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9cd838-ded1-4c2d-aa36-007aea26767b",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": [
    "# create a table for the customers and set the \n",
    "# customer_id as a candidate key (candidate and \n",
    "# foreign keys are used to construct the edges of the graph)\n",
    "# Candidate keys can be thought as primary keys. A GNNTable can\n",
    "# have more than one candidate keys. Candidate keys are valid keys\n",
    "# as long as each key uniquely identifies all rows in a table.\n",
    "\n",
    "# GNN tables can be of two types, either \"node\" or \"edge\".\n",
    "# Edge tables are modeled directly as edges in the GNN and\n",
    "# do not construct nodes. Edge tables can only have two columns\n",
    "# (e.g., from/to)that specify the foreign keys that connect to \n",
    "# other tables\n",
    "customers_table = GNNTable(\n",
    "    type = \"node\",\n",
    "    connector=connector,\n",
    "    source=CUSTOMER_SOURCE_TABLE, # specify path to the database to load the table\n",
    "    name=CUSTOMER_NAME, # name of table can be anything, we use it as a reference to the table name\n",
    "    candidate_keys = [CandidateKey(column_name=CUSTOMER_CANDIDATE_KEY)],\n",
    ")\n",
    "customers_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de06867-2433-44db-a532-76dd5044fd5f",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": [
    "# in a similar manner we can create the ARTICLES table\n",
    "articles_table = GNNTable(\n",
    "    type = 'node',\n",
    "    connector=connector,\n",
    "    source=ARTICLE_SOURCE_TABLE,\n",
    "    name=ARTICLE_NAME,\n",
    "    candidate_keys =[CandidateKey(column_name = ARTICLE_CANDIDATE_KEY)],\n",
    ")\n",
    "articles_table.show_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f426a-0063-4e14-aa5e-f5e34e91cb0b",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "# and finally we will link the two tables using the foreign\n",
    "# keys from the TRANSACTIONS table.\n",
    "# Note that the date column (`t_dat`) is important. \n",
    "# In a typical scenario where one would use shallow models (e.g,. XGBoost) \n",
    "# to construct a feature matrix, special care \n",
    "# needs to be taken to avoid time leakage \n",
    "# (for example, by masking future transactions). \n",
    "# By specifying the date column as a time column, \n",
    "# GNNs take care of the time leakage problems.\n",
    "\n",
    "transactions_table = GNNTable(\n",
    "    type = 'node',\n",
    "    connector=connector,\n",
    "    source=TRANSACTION_SOURCE_TABLE,\n",
    "    name=TRANSACTION_NAME,\n",
    "    foreign_keys=[\n",
    "        ForeignKey(\n",
    "            column_name=CUSTOMER_CANDIDATE_KEY, link_to=CUSTOMER_NAME+\".\"+CUSTOMER_CANDIDATE_KEY),\n",
    "        ForeignKey(\n",
    "            column_name=ARTICLE_CANDIDATE_KEY, link_to=ARTICLE_NAME+\".\"+ARTICLE_CANDIDATE_KEY),\n",
    "    ],\n",
    "    time_column=TIME_COLUMN,\n",
    ")\n",
    "transactions_table.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1961b27-cac3-441d-90fa-b0ae1495faff",
   "metadata": {
    "collapsed": false,
    "name": "cell18"
   },
   "source": [
    "\n",
    "## ðŸ”§ Preparing the Data: Creating the Task\n",
    "\n",
    "To define the task, we begin by specifying the locations of the training, validation, and test datasets. We also identify the source and destination entity tables, along with the corresponding columns that uniquely identify each entity.\n",
    "\n",
    "Since this is a **link prediction** task, our objective is to predict future connections between a source entity and a destination entity.\n",
    "\n",
    "Additionally, we define a **timestamp column** to avoid information leakage by ensuring that future data doesn't influence past predictions. Lastly, we specify the **evaluation metric**â€”in this case, **Mean Average Precision (MAP)**â€”to assess the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000011",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": [
    "link_pred_task = LinkTask(\n",
    "    connector=connector,\n",
    "    name=LINK_TASK_NAME,\n",
    "    task_data_source={\n",
    "        \"train\": TASK_TRAIN_TABLE, \n",
    "        \"test\": TASK_TEST_TABLE, \n",
    "        \"validation\": TASK_VALIDATION_TABLE\n",
    "    },    \n",
    "    # the source entity column is a foreign key that points to the\n",
    "    # GNNTable that holds the information of the source entity colum.\n",
    "    # The name of the key corresponds to the task column name that holds\n",
    "    # the source entity. \n",
    "    source_entity_column=ForeignKey(\n",
    "        column_name = TASK_SOURCE_ENTITY_COLUMN_NAME, link_to = TASK_SOURCE_ENTITY_COLUMN_LINKTO\n",
    "    ),    \n",
    "    # similarly we will define the target entity column\n",
    "    target_entity_column=ForeignKey(\n",
    "        column_name = TASK_TARGET_ENTITY_COLUMN_NAME, link_to = TASK_TARGET_ENTITY_COLUMN_LINKTO\n",
    "    ),\n",
    "    # name of the time column in the task table    \n",
    "    time_column=TASK_TIME_COLUMN_NAME,\n",
    "    task_type=TaskType.LINK_PREDICTION,\n",
    "    evaluation_metric=EvaluationMetric(name=\"link_prediction_map\", eval_at_k=12),\n",
    ")\n",
    "\n",
    "link_pred_task.show_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfe70e-475a-4fe7-8260-ce08b43ec297",
   "metadata": {
    "collapsed": false,
    "name": "cell20"
   },
   "source": [
    "## ðŸ§© Preparing the Data: Creating the Dataset\n",
    "\n",
    "Finally, we combine all the components by constructing a dataset object that encapsulates both the GNN tables and the task definition. This dataset will serve as the input to the model training pipeline, ensuring that the task and its associated data are tightly integrated and ready for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000013",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    connector=connector,\n",
    "    dataset_name=DATASET_NAME,\n",
    "    tables=[articles_table, customers_table, transactions_table],\n",
    "    task_description=link_pred_task,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000014",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# we can also visualize the dataset \n",
    "graph = dataset.visualize_dataset(show_dtypes=True)\n",
    "# play with font size and plot size to get a good visualization\n",
    "for node in graph.get_nodes():    \n",
    "    font_size = node.get_attributes()['fontsize']\n",
    "    font_size = \"16\"\n",
    "    node.set('fontsize', font_size)\n",
    "\n",
    "graph.set_graph_defaults(size=\"10,10!\")  # Increase graph size\n",
    "\n",
    "src = Source(graph.to_string())\n",
    "src  # Display in notebo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893e1161-f0f7-4d1a-83fe-07cb7c340562",
   "metadata": {
    "collapsed": false,
    "name": "cell23"
   },
   "source": [
    "## ðŸš€ GNN Model Training\n",
    "\n",
    "Now that our dataset is ready, we can train our first GNN model. Weâ€™ll begin by defining a **configuration** that specifies the training parameters, such as model architecture, optimizer settings, and training duration.\n",
    "\n",
    "Next, weâ€™ll instantiate a **trainer** using this configuration. The trainer will consume the dataset we previously created and manage the entire training process. By calling the `fit()` method on the trainer, we initiate a training jobâ€”whose progress and status can be monitored throughout execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce110000-1111-2222-3333-ffffff000016",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": [
    "# the first step will be to define a configuration for our Trainer.\n",
    "# the configuration includes many parameters that are explained in\n",
    "# detail in the documentation. It does not only provide parameters\n",
    "# for the graph neural network but also parameters for other components\n",
    "# of the model (such as feature extractors, prediction head parameters,\n",
    "# training parameters etc.)\n",
    "model_config = TrainerConfig(\n",
    "    connector=connector,\n",
    "    device=MODEL_DEVICE,  # either 'cuda' or 'cpu'\n",
    "    n_epochs=MODEL_N_EPOCHS,\n",
    "    max_iters=MODEL_MAX_ITERS,\n",
    "    text_embedder=MODEL_TEXT_EMBEDDER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f9f2a-4036-4ae0-bfa9-8c96682cdb59",
   "metadata": {
    "language": "python",
    "name": "cell25"
   },
   "outputs": [],
   "source": [
    "# we initialize now our trainer object with the trainer configuration\n",
    "# the trainer object can be used to train a model, to perform inference\n",
    "# or to perform training & inference.\n",
    "trainer = Trainer(connector=connector, config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f2d2f-eba9-4282-bc84-7731093ced4b",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "# in our first example we will use the trainer to perform training only.\n",
    "# every time the trainer is \"executed\" (calling fit(), predict() or fit_predict())\n",
    "# it returns a job object that can be used to monitor the current running job.\n",
    "# See the documentation for the meaning of the job statuses\n",
    "train_job = trainer.fit(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c0738-e489-4d97-8b01-04bc1fff4b22",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": [
    "# we can also stream the logs of the training job in real time\n",
    "# Hint: You can stop the cell execution to stop monitoring of logs\n",
    "# Hint: At this point you can also open the MLFLow URL to monitor your experiments\n",
    "\n",
    "# Training will take approximately 9 to 10 minutes to complete.\n",
    "train_job.stream_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ad33b-5eee-484c-a5a0-29d76cd3c088",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "# hint: one can cancel a running job as well\n",
    "# train_job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d2d96-9185-45f5-9efc-19e4f8fa6873",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "# now we can monitor the job status\n",
    "# observe that once the job is running we also get back an experiment name\n",
    "# we will see later how we can use that to perform inference\n",
    "train_job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a015d1-8eec-4d76-b409-0b2eaef2f897",
   "metadata": {
    "collapsed": false,
    "name": "cell32"
   },
   "source": [
    "## ðŸ” Inference Using a Trained Model\n",
    "\n",
    "Finally, weâ€™ll demonstrate how to perform inference using the model weâ€™ve just trained. In this example, we'll directly use the recently trained model to generate predictions.\n",
    "\n",
    "For more advanced use casesâ€”such as registering a model for reuse or automatically selecting the best-performing modelâ€”please refer to the churn prediction notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c715ba-1895-490b-9d4a-64fe9c947931",
   "metadata": {
    "collapsed": false,
    "name": "cell42"
   },
   "source": [
    "âš ï¸ **Warning:** Existing Snowflake tables are never overwritten. To run inference and save predictions to a Snowflake table, you must either:\n",
    "\n",
    "* Change the OUTPUT_TABLE to a new, non-existent table, or\n",
    "\n",
    "* Delete the existing OUTPUT_TABLE (if you have the necessary permissions). You can do this by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87c0eb-c76e-486e-9a52-346762b2993f",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": [
    "# Note: We never overwrite already existing snowflake tables.\n",
    "# remove the previous predictions, if the table exists\n",
    "df = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\"); \n",
    "if (len(df.collect()) > 0): # table exists\n",
    "    df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_NAME}.PUBLIC.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n",
    "    df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_NAME}.PUBLIC.{OUTPUT_TABLE};\") ; df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593de278-2e2a-4421-8726-eca08c4c822e",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n",
    "# make sure that the table with the same alias does not already exist\n",
    "# we never overwrite tables\n",
    "\n",
    "# When materialize_results is True (default), the method waits for the job \n",
    "# to complete and creates permanent tables.\n",
    "# This causes the method to run synchronously (blocking) \n",
    "# rather than executing in the background.\n",
    "\n",
    "inference_job = trainer.predict(\n",
    "    output_alias=OUTPUT_ALIAS,\n",
    "    output_config=output_config,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    dataset=dataset,\n",
    "    model_run_id=train_job.model_run_id,\n",
    "    extract_embeddings=True,\n",
    "    materialize_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bb867-680d-486f-8060-0a197036618d",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "inference_job.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372221b2-2fe8-4804-99b0-bf8bfb70c5da",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": [
    "# Finally, let's take a look at some of the predictions done by our GNN model\n",
    "# These predictions are saved in the OUTPUT_TABLE \n",
    "\n",
    "df = provider._session.sql(f\"SELECT * FROM {OUTPUT_TABLE} LIMIT 100;\") ; df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b754be8-7004-4216-b85d-a00358573ed3",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": [
    "## ðŸ“‹ Job Manager\n",
    "\n",
    "It might be the case that we have lost track of the jobs that we are running. To this end we also provide to the user a JobManager object that can give us the status of all jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a364f3c-0ed1-4f03-8ec9-349d93100dd3",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell30"
   },
   "outputs": [],
   "source": [
    "# Let's see an example:\n",
    "job_manager = JobManager(connector=connector)\n",
    "job_manager.show_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcbeb6-55ea-4242-a343-e31accac11e5",
   "metadata": {
    "language": "python",
    "name": "cell31"
   },
   "outputs": [],
   "source": [
    "# You can retrieve job details using its job ID.\n",
    "# For example, use the job ID of the training job \n",
    "# you see above to access its details\n",
    "# and use the trained model for inference.\n",
    "\n",
    "# NOTE: To run this cell, replace the job ID below with the \n",
    "# actual ID from your training job shown in the previous cell.\n",
    "\n",
    "# hint: the job manager can be used to cancel any job as well\n",
    "\n",
    "# retrieved_job = job_manager.fetch_job(\"USE YOUR JOB ID HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a96893-faf2-4eab-97bd-e6d7e3aaf58c",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "# Note: We never overwrite already existing snowflake tables.\n",
    "# remove the previous predictions, if the table exists\n",
    "\n",
    "# df = provider._session.sql(f\"SELECT * FROM {DB_NAME}.information_schema.tables WHERE table_name = '{OUTPUT_TABLE}';\");\n",
    "# if (len(df.collect()) > 0): # table exists\n",
    "#     df = provider._session.sql(f\"GRANT OWNERSHIP ON {DB_NAME}.PUBLIC.{OUTPUT_TABLE} TO ROLE ACCOUNTADMIN REVOKE CURRENT GRANTS;\") ; df.collect()\n",
    "#     df = provider._session.sql(f\"DROP TABLE IF EXISTS {DB_NAME}.PUBLIC.{OUTPUT_TABLE};\") ; df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98566e-19f4-4634-92b0-90edc0a2e4a3",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": [
    "# output_config = OutputConfig.snowflake(database_name=DB_NAME, schema_name=\"PUBLIC\")\n",
    "\n",
    "# make sure that the table with the same alias does not already exist\n",
    "# we never overwrite tables\n",
    "# use the retrieved job to get the trained model for inference\n",
    "\n",
    "# inference_job_2 = trainer.predict(\n",
    "#     output_alias=OUTPUT_ALIAS,\n",
    "#     output_config=output_config,\n",
    "#     test_batch_size=TEST_BATCH_SIZE,\n",
    "#     dataset=dataset,\n",
    "#     model_run_id=retrieved_job.model_run_id,\n",
    "#     extract_embeddings=False,\n",
    "#     materialize=False\n",
    "# )\n",
    "\n",
    "# inference_job_2.stream_logs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdk_ht",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "lastEditStatus": {
   "authorEmail": "dafni.anagnostopoulou@relational.ai",
   "authorId": "5493200917234",
   "authorName": "dafni.anagnostopoulou@relational.ai",
   "lastEditTime": 1755598716865,
   "notebookId": "237blqvoz53fftre5plt",
   "sessionId": "c1c129d5-f0c7-4377-902b-f090114a15eb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
